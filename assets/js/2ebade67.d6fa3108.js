"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[9347],{7731:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>r,contentTitle:()=>l,default:()=>d,frontMatter:()=>a,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module4-vla/lesson3","title":"Lesson 3: Building a Vision-Language-Action Pipeline","description":"In this final lesson, we will bring together the concepts we have learned throughout this book to outline how a complete Vision-Language-Action (VLA) pipeline can be built for a robot. This pipeline will enable a robot to take a natural language command, perceive its environment, and execute the requested task.","source":"@site/docs/module4-vla/lesson3.md","sourceDirName":"module4-vla","slug":"/module4-vla/lesson3","permalink":"/hackathone-physical-AI-humonoid-robtics--Book/book/module4-vla/lesson3","draft":false,"unlisted":false,"editUrl":"https://github.com/SaimaWaheed-Student/hackathone-physical-AI-humonoid-robtics--Book/tree/main/website/docs/module4-vla/lesson3.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"bookSidebar","previous":{"title":"Lesson 2: Introduction to CLIP and Grounding DINO","permalink":"/hackathone-physical-AI-humonoid-robtics--Book/book/module4-vla/lesson2"}}');var i=t(4848),s=t(8453);const a={sidebar_position:3},l="Lesson 3: Building a Vision-Language-Action Pipeline",r={},c=[{value:"The VLA Pipeline at a High Level",id:"the-vla-pipeline-at-a-high-level",level:2},{value:"1. Task Decomposition with a Large Language Model (LLM)",id:"1-task-decomposition-with-a-large-language-model-llm",level:3},{value:"2. Object Recognition with Grounding DINO and CLIP",id:"2-object-recognition-with-grounding-dino-and-clip",level:3},{value:"3. Motion Planning",id:"3-motion-planning",level:3},{value:"4. Execution and Feedback",id:"4-execution-and-feedback",level:3},{value:"Putting It All Together with ROS 2",id:"putting-it-all-together-with-ros-2",level:2},{value:"The Future is Here",id:"the-future-is-here",level:2}];function h(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"lesson-3-building-a-vision-language-action-pipeline",children:"Lesson 3: Building a Vision-Language-Action Pipeline"})}),"\n",(0,i.jsx)(n.p,{children:"In this final lesson, we will bring together the concepts we have learned throughout this book to outline how a complete Vision-Language-Action (VLA) pipeline can be built for a robot. This pipeline will enable a robot to take a natural language command, perceive its environment, and execute the requested task."}),"\n",(0,i.jsx)(n.h2,{id:"the-vla-pipeline-at-a-high-level",children:"The VLA Pipeline at a High Level"}),"\n",(0,i.jsx)(n.p,{children:"A typical VLA pipeline can be broken down into the following stages:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Task Decomposition"}),': A high-level natural language command (e.g., "clean up the table") is broken down into a sequence of simpler, actionable steps.']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Object Recognition and Grounding"}),": The robot identifies and locates the objects relevant to the current step in its environment."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Motion Planning"}),": The robot plans the physical movements required to execute the step."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Execution and Feedback"}),": The robot executes the planned motion and uses feedback from its sensors to ensure the action is successful."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Let's look at each of these stages in more detail."}),"\n",(0,i.jsx)(n.h3,{id:"1-task-decomposition-with-a-large-language-model-llm",children:"1. Task Decomposition with a Large Language Model (LLM)"}),"\n",(0,i.jsx)(n.p,{children:'The process starts with a high-level command from a user. This command might be too abstract for the robot to execute directly. We can use a powerful Large Language Model (LLM), like GPT-4, to act as a "task decomposer".'}),"\n",(0,i.jsx)(n.p,{children:"The LLM would take the high-level command and the current state of the world (e.g., a list of objects on the table) as input and output a sequence of simpler sub-tasks."}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Example"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"User Command"}),': "Please get me the apple from the kitchen."']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"LLM Output (sub-tasks)"}),":","\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Navigate to the kitchen."}),"\n",(0,i.jsx)(n.li,{children:"Find the apple."}),"\n",(0,i.jsx)(n.li,{children:"Pick up the apple."}),"\n",(0,i.jsx)(n.li,{children:"Navigate back to the user."}),"\n",(0,i.jsx)(n.li,{children:"Place the apple in front of the user."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"2-object-recognition-with-grounding-dino-and-clip",children:"2. Object Recognition with Grounding DINO and CLIP"}),"\n",(0,i.jsx)(n.p,{children:"For each sub-task, the robot needs to perceive its environment. This is where models like Grounding DINO and CLIP come in."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Grounding DINO"}),' can be used to detect the objects mentioned in the sub-task (e.g., "find the apple"). It will provide a bounding box for the object in the robot\'s camera image.']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"CLIP"}),' can be used to resolve any ambiguity. For example, if there are multiple apples, CLIP can help identify the correct one based on a more descriptive phrase (e.g., "the green apple").']}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"3-motion-planning",children:"3. Motion Planning"}),"\n",(0,i.jsxs)(n.p,{children:["Once the robot knows where the target object is, it needs to plan a path to it and a grasping motion to pick it up. This is the domain of motion planning. ROS 2 provides powerful motion planning frameworks like ",(0,i.jsx)(n.strong,{children:"MoveIt 2"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"MoveIt 2 can take the target pose of the robot's end-effector (the gripper) as input and generate a collision-free trajectory for the robot's arm."}),"\n",(0,i.jsx)(n.h3,{id:"4-execution-and-feedback",children:"4. Execution and Feedback"}),"\n",(0,i.jsx)(n.p,{children:"The planned trajectory is then sent to the robot's controllers for execution. Throughout the execution, the robot uses its sensors (e.g., joint encoders, force-torque sensors) to monitor the progress of the action. This feedback is crucial for ensuring that the action is performed correctly and for reacting to unexpected events."}),"\n",(0,i.jsx)(n.h2,{id:"putting-it-all-together-with-ros-2",children:"Putting It All Together with ROS 2"}),"\n",(0,i.jsx)(n.p,{children:"ROS 2 is the perfect framework for orchestrating this entire pipeline. Each of the components we have discussed can be implemented as a separate ROS 2 node:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["A ",(0,i.jsx)(n.code,{children:"task_decomposer_node"})," (using an LLM)."]}),"\n",(0,i.jsxs)(n.li,{children:["A ",(0,i.jsx)(n.code,{children:"object_detector_node"})," (using Grounding DINO and CLIP)."]}),"\n",(0,i.jsxs)(n.li,{children:["A ",(0,i.jsx)(n.code,{children:"motion_planner_node"})," (using MoveIt 2)."]}),"\n",(0,i.jsx)(n.li,{children:"The robot's own control and sensor nodes."}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"These nodes would communicate with each other using ROS 2 topics, services, and actions to create a flexible and powerful robotics system."}),"\n",(0,i.jsx)(n.h2,{id:"the-future-is-here",children:"The Future is Here"}),"\n",(0,i.jsx)(n.p,{children:"Congratulations on completing this book! You have journeyed from the fundamentals of ROS 2 to the cutting edge of AI in robotics. The field of Vision-Language-Action models is evolving at an incredible pace, and the tools and techniques you have learned here will provide a solid foundation for building the next generation of intelligent robots."})]})}function d(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>l});var o=t(6540);const i={},s=o.createContext(i);function a(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);