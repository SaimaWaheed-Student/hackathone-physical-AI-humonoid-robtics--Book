"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[2015],{1993:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>r,default:()=>h,frontMatter:()=>a,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"module4-vla/lesson2","title":"Lesson 2: Introduction to CLIP and Grounding DINO","description":"In the previous lesson, we introduced the concept of Vision-Language-Action (VLA) models. In this lesson, we will dive deeper into two important models that are often used as building blocks in VLA pipelines: CLIP and Grounding DINO.","source":"@site/docs/module4-vla/lesson2.md","sourceDirName":"module4-vla","slug":"/module4-vla/lesson2","permalink":"/hackathone-physical-AI-humonoid-robtics--Book/book/module4-vla/lesson2","draft":false,"unlisted":false,"editUrl":"https://github.com/SaimaWaheed-Student/hackathone-physical-AI-humonoid-robtics--Book/tree/main/website/docs/module4-vla/lesson2.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"bookSidebar","previous":{"title":"Lesson 1: Introduction to Vision-Language-Action Models","permalink":"/hackathone-physical-AI-humonoid-robtics--Book/book/module4-vla/lesson1"},"next":{"title":"Lesson 3: Building a Vision-Language-Action Pipeline","permalink":"/hackathone-physical-AI-humonoid-robtics--Book/book/module4-vla/lesson3"}}');var t=i(4848),s=i(8453);const a={sidebar_position:2},r="Lesson 2: Introduction to CLIP and Grounding DINO",d={},l=[{value:"CLIP (Contrastive Language-Image Pre-Training)",id:"clip-contrastive-language-image-pre-training",level:2},{value:"How does CLIP work?",id:"how-does-clip-work",level:3},{value:"What can CLIP be used for?",id:"what-can-clip-be-used-for",level:3},{value:"Grounding DINO",id:"grounding-dino",level:2},{value:"How does Grounding DINO work?",id:"how-does-grounding-dino-work",level:3},{value:"Why is this useful for robotics?",id:"why-is-this-useful-for-robotics",level:3},{value:"Combining CLIP and Grounding DINO",id:"combining-clip-and-grounding-dino",level:2},{value:"Next Steps",id:"next-steps",level:2}];function c(e){const n={h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"lesson-2-introduction-to-clip-and-grounding-dino",children:"Lesson 2: Introduction to CLIP and Grounding DINO"})}),"\n",(0,t.jsx)(n.p,{children:"In the previous lesson, we introduced the concept of Vision-Language-Action (VLA) models. In this lesson, we will dive deeper into two important models that are often used as building blocks in VLA pipelines: CLIP and Grounding DINO."}),"\n",(0,t.jsx)(n.h2,{id:"clip-contrastive-language-image-pre-training",children:"CLIP (Contrastive Language-Image Pre-Training)"}),"\n",(0,t.jsx)(n.p,{children:'CLIP is a neural network developed by OpenAI that is trained on a massive dataset of images and their corresponding text descriptions. The key innovation of CLIP is that it learns to associate images with text in a shared "embedding space". This means that an image of a dog and the text "a photo of a dog" will have very similar representations in CLIP\'s embedding space.'}),"\n",(0,t.jsx)(n.h3,{id:"how-does-clip-work",children:"How does CLIP work?"}),"\n",(0,t.jsx)(n.p,{children:"CLIP is trained using a contrastive learning objective. During training, the model is given a batch of image-text pairs. For each pair, the model tries to predict which text description corresponds to which image. It does this by maximizing the similarity between the embeddings of the correct image-text pairs and minimizing the similarity between the embeddings of incorrect pairs."}),"\n",(0,t.jsx)(n.h3,{id:"what-can-clip-be-used-for",children:"What can CLIP be used for?"}),"\n",(0,t.jsx)(n.p,{children:"CLIP is incredibly versatile. Some common applications include:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Zero-shot image classification"}),': You can classify images into categories that the model has never seen before. For example, you can give CLIP an image and a set of text descriptions (e.g., "a photo of a cat", "a photo of a dog") and it will tell you which description best matches the image.']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Image search"}),": You can use CLIP to search for images using natural language queries."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Object detection"}),": While not its primary purpose, CLIP can be used in combination with other techniques for open-vocabulary object detection."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"grounding-dino",children:"Grounding DINO"}),"\n",(0,t.jsx)(n.p,{children:'Grounding DINO is an open-set object detection model. "Open-set" means that it can detect objects that it was not explicitly trained to recognize. It does this by "grounding" natural language queries in the image.'}),"\n",(0,t.jsx)(n.h3,{id:"how-does-grounding-dino-work",children:"How does Grounding DINO work?"}),"\n",(0,t.jsx)(n.p,{children:'Grounding DINO combines a transformer-based object detector (DINO) with a language model. You provide it with an image and a text prompt describing the object you want to detect (e.g., "a red car"). The model then uses the language prompt to guide its attention and identify the object in the image, outputting a bounding box for it.'}),"\n",(0,t.jsx)(n.h3,{id:"why-is-this-useful-for-robotics",children:"Why is this useful for robotics?"}),"\n",(0,t.jsx)(n.p,{children:'For a robot to interact with its environment based on language commands, it first needs to be able to identify the objects mentioned in the command. Grounding DINO is a powerful tool for this purpose. If you tell a robot to "pick up the banana", Grounding DINO can be used to find the location of the banana in the robot\'s camera view.'}),"\n",(0,t.jsx)(n.h2,{id:"combining-clip-and-grounding-dino",children:"Combining CLIP and Grounding DINO"}),"\n",(0,t.jsx)(n.p,{children:"CLIP and Grounding DINO are often used together in robotics pipelines. For example, you might use Grounding DINO to detect all the objects in a scene, and then use CLIP to classify each detected object and determine which one best matches the user's command."}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsx)(n.p,{children:"In the final lesson of this book, we will look at how these components can be assembled to build a complete Vision-Language-Action pipeline."})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>r});var o=i(6540);const t={},s=o.createContext(t);function a(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);